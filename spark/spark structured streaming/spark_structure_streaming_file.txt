from pyspark import SparkConf, SparkContext 
from pyspark.sql import SparkSession
from pyspark.sql import SQLContext 
from pyspark.sql.functions import *
from pyspark.sql.types import *


hadoop fs -rm -r /stream/

stream directory : hadoop fs -mkdir /stream/

hadoop fs -put custs_v1.txt  /stream/

hadoop fs -put custs_v2.txt  /stream/

hadoop fs -put custs_v3.txt  /stream/


spark = SparkSession \
    .builder \
    .appName("user-logs-analysis-streaming") \
    .getOrCreate()

schema1 = StructType([ \
        StructField("custid", IntegerType(), True), \
        StructField("custfname", StringType(), True), \
        StructField("custlname", StringType(), True), \
        StructField("custage", StringType(), True), \
        StructField("custprofession", StringType(), True), \
        ])

#.option("checkpointLocation", "/checkpoint") \

df = spark \
      .readStream \
      .format("csv") \
      .option("header", "true") \
      .option("inferSchema", True) \
      .option("maxFilesPerTrigger", 1) \
      .option("checkpointLocation", "/checkpoint") \
      .schema(schema1) \
      .load("/stream/")

df.printSchema()

df.show(10,False)

case1:

stream_data=df \
    .writeStream \
    .format("console") \
    .outputMode("append") \
    .start()

stream_data.awaitTermination()

case 2:

df2=df.groupBy("custprofession").agg(avg("custage").alias("avg_age"), count("custid").alias("customer_count"))

stream_data=df2 \
    .writeStream \
    .format("console") \
    .outputMode("complete") \
    .trigger(processingTime="10 seconds") \
    .start().awaitTermination()

stream_data.awaitTermination()


#.trigger(Trigger.ProcessingTime("1 minute"))

#.trigger(Trigger.Once())


