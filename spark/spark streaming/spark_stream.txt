from pyspark import SparkContext
# import the spark streaming package to enable the support of streaming 
from pyspark.streaming import StreamingContext

#creat an object for streaming context including the sparkContext 
# ,1 indicates the trigger interval for the micro batches 

ssc = StreamingContext(sc, 5)

# ensure to change the hostname to your cluster Name 
# how to find hostname : just type hostname in terminal on dataproc. 
lines = ssc.socketTextStream("cluster-796e-m", 9999)


# regular RDD operations 
words = lines.flatMap(lambda line: line.split(" "))
pairs = words.map(lambda word: (word, 1))
wordCounts = pairs.reduceByKey(lambda x, y: x + y)
wordCounts.pprint()

# start and stop the operatins. 
ssc.start()             # Start the computation
ssc.awaitTermination()  # Wait for the computation to terminate


#open a new window:
#---------------------
#Steam the data from linux network socket.
#-------------------------------------------
command to opent the socket:nc -lk 9999


sample data: [paste it into the socket termainal]
hello world any text you want


spark-submit --master yarn  /home/Raj/bin/customer_stream.py